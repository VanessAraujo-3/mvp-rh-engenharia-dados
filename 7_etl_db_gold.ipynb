{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20891fce-6c99-437d-9230-71851a1bc82c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MVP Pipeline de Dados – Recursos Humanos  \n",
    "## Análise de Retenção e Eficiência Organizacional  \n",
    "\n",
    "**Vanessa Araújo**  \n",
    "\n",
    "PUC-RJ – MBA em Ciência de Dados e Analytics  \n",
    "Disciplina de Engenharia de Dados  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0502861-5a0e-4ec8-90df-8b8f8279530a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Script ETL para carga na camada GOLD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "873efdec-aed6-492b-acb2-e8a049858e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Carga de Dados\n",
    "Os dados serão carregados a partir da camada silver gerando as métricas a serem persistidas na camada gold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6454829-0b6d-493d-90eb-a761e6db9d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. gold.turnover_metrics — Rotatividade**\n",
    "\n",
    "**Objetivo:** Analisar funcionários ativos vs desligados e calcular taxa de rotatividade por departamento.\n",
    "\n",
    "**Estrutura da tabela:**\n",
    "\n",
    "department\n",
    "\n",
    "total_funcionarios\n",
    "\n",
    "total_desligados\n",
    "\n",
    "taxa_rotatividade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea1a953-8b4f-4cb1-b62a-981c29248542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6774941890420307>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCREATE OR REPLACE TABLE gold.turnover_metrics AS\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mSELECT\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    d.department_name AS department,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    COUNT(DISTINCT f.employee_id) AS total_funcionarios,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    SUM(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        CASE \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            WHEN f.termination_date IS NOT NULL THEN 1 \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            ELSE 0 \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        END\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    ) AS total_desligados,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    ROUND(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        SUM(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            CASE \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                WHEN f.termination_date IS NOT NULL THEN 1 \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                ELSE 0 \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            END\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        ) / COUNT(DISTINCT f.employee_id),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        4\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    ) AS taxa_rotatividade,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n",
       "\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n",
       "\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n",
       "\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n",
       "\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n",
       "\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n",
       "\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:194\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    189\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n",
       "\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    191\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n",
       "\u001B[1;32m    192\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    193\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:187\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    185\u001B[0m         query_text \u001B[38;5;241m=\u001B[39m sub_query\u001B[38;5;241m.\u001B[39mquery()\n",
       "\u001B[1;32m    186\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n",
       "\u001B[0;32m--> 187\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    189\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n",
       "\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    191\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n",
       "\u001B[1;32m    192\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m    193\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:206\u001B[0m, in \u001B[0;36mSqlMagic._handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m     query \u001B[38;5;241m=\u001B[39m sql_directive\u001B[38;5;241m.\u001B[39msql()\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_udf_if_needed(query)\n",
       "\u001B[0;32m--> 206\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_query_request_result(query)\n",
       "\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m directive_name \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreateView\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDropTable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAlterTable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreateTable\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[1;32m    208\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_query_request_result(sql_directive\u001B[38;5;241m.\u001B[39msql())\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:151\u001B[0m, in \u001B[0;36mSqlMagic._get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n",
       "\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(widget_bindings \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_widget_cache\u001B[38;5;241m.\u001B[39mvalues) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m    150\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPARAM_SYNTAX_USAGE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 151\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39msql(query, widget_bindings)\n",
       "\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/session.py:875\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    872\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n",
       "\u001B[1;32m    874\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n",
       "\u001B[0;32m--> 875\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n",
       "\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n",
       "\u001B[1;32m    877\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1558\u001B[0m )\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mParseException\u001B[0m: \n",
       "[PARSE_SYNTAX_ERROR] Syntax error at or near end of input. SQLSTATE: 42601 (line 21, pos 27)\n",
       "\n",
       "== SQL ==\n",
       "CREATE OR REPLACE TABLE gold.turnover_metrics AS\n",
       "SELECT\n",
       "    d.department_name AS department,\n",
       "    COUNT(DISTINCT f.employee_id) AS total_funcionarios,\n",
       "    \n",
       "    SUM(\n",
       "        CASE \n",
       "            WHEN f.termination_date IS NOT NULL THEN 1 \n",
       "            ELSE 0 \n",
       "        END\n",
       "    ) AS total_desligados,\n",
       "    \n",
       "    ROUND(\n",
       "        SUM(\n",
       "            CASE \n",
       "                WHEN f.termination_date IS NOT NULL THEN 1 \n",
       "                ELSE 0 \n",
       "            END\n",
       "        ) / COUNT(DISTINCT f.employee_id),\n",
       "        4\n",
       "    ) AS taxa_rotatividade,\n",
       "---------------------------^^^\n",
       "\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.parser.ParseException\n",
       "\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:479)\n",
       "\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:120)\n",
       "\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:167)\n",
       "\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$7(SparkSession.scala:831)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$6(SparkSession.scala:831)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:827)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:826)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3811)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3635)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3458)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ParseException",
        "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input. SQLSTATE: 42601 (line 21, pos 27)\n\n== SQL ==\nCREATE OR REPLACE TABLE gold.turnover_metrics AS\nSELECT\n    d.department_name AS department,\n    COUNT(DISTINCT f.employee_id) AS total_funcionarios,\n    \n    SUM(\n        CASE \n            WHEN f.termination_date IS NOT NULL THEN 1 \n            ELSE 0 \n        END\n    ) AS total_desligados,\n    \n    ROUND(\n        SUM(\n            CASE \n                WHEN f.termination_date IS NOT NULL THEN 1 \n                ELSE 0 \n            END\n        ) / COUNT(DISTINCT f.employee_id),\n        4\n    ) AS taxa_rotatividade,\n---------------------------^^^\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.parser.ParseException\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:479)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:120)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:167)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$7(SparkSession.scala:831)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$6(SparkSession.scala:831)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:827)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:826)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3811)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3635)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3458)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       },
       "metadata": {
        "errorSummary": "[PARSE_SYNTAX_ERROR] Syntax error at or near end of input. SQLSTATE: 42601"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "PARSE_SYNTAX_ERROR",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "42601",
        "stackTrace": "org.apache.spark.sql.catalyst.parser.ParseException\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:479)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:120)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:167)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$7(SparkSession.scala:831)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$6(SparkSession.scala:831)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:827)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:826)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3811)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3635)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3458)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-6774941890420307>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCREATE OR REPLACE TABLE gold.turnover_metrics AS\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mSELECT\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    d.department_name AS department,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    COUNT(DISTINCT f.employee_id) AS total_funcionarios,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    SUM(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        CASE \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            WHEN f.termination_date IS NOT NULL THEN 1 \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            ELSE 0 \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        END\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    ) AS total_desligados,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    ROUND(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        SUM(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            CASE \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                WHEN f.termination_date IS NOT NULL THEN 1 \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                ELSE 0 \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            END\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        ) / COUNT(DISTINCT f.employee_id),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        4\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    ) AS taxa_rotatividade,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:194\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    191\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    192\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    193\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:187\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    185\u001B[0m         query_text \u001B[38;5;241m=\u001B[39m sub_query\u001B[38;5;241m.\u001B[39mquery()\n\u001B[1;32m    186\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n\u001B[0;32m--> 187\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    191\u001B[0m         exceptionClassName\u001B[38;5;241m=\u001B[39me\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    192\u001B[0m         sqlState\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetSqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    193\u001B[0m         errorClass\u001B[38;5;241m=\u001B[39msafe_call(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgetCondition\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:206\u001B[0m, in \u001B[0;36mSqlMagic._handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n\u001B[1;32m    204\u001B[0m     query \u001B[38;5;241m=\u001B[39m sql_directive\u001B[38;5;241m.\u001B[39msql()\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_udf_if_needed(query)\n\u001B[0;32m--> 206\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_query_request_result(query)\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m directive_name \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreateView\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDropTable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAlterTable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreateTable\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_query_request_result(sql_directive\u001B[38;5;241m.\u001B[39msql())\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:151\u001B[0m, in \u001B[0;36mSqlMagic._get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(widget_bindings \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_widget_cache\u001B[38;5;241m.\u001B[39mvalues) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPARAM_SYNTAX_USAGE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 151\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39msql(query, widget_bindings)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/session.py:875\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m    872\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n\u001B[1;32m    874\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n\u001B[0;32m--> 875\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m    877\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mParseException\u001B[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input. SQLSTATE: 42601 (line 21, pos 27)\n\n== SQL ==\nCREATE OR REPLACE TABLE gold.turnover_metrics AS\nSELECT\n    d.department_name AS department,\n    COUNT(DISTINCT f.employee_id) AS total_funcionarios,\n    \n    SUM(\n        CASE \n            WHEN f.termination_date IS NOT NULL THEN 1 \n            ELSE 0 \n        END\n    ) AS total_desligados,\n    \n    ROUND(\n        SUM(\n            CASE \n                WHEN f.termination_date IS NOT NULL THEN 1 \n                ELSE 0 \n            END\n        ) / COUNT(DISTINCT f.employee_id),\n        4\n    ) AS taxa_rotatividade,\n---------------------------^^^\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.parser.ParseException\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:479)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:120)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:167)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:118)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$7(SparkSession.scala:831)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$6(SparkSession.scala:831)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:738)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:827)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:826)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3811)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3635)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3458)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:281)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:532)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:532)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:531)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE gold.turnover_metrics AS\n",
    "SELECT\n",
    "    d.department_name AS department,\n",
    "    COUNT(DISTINCT f.employee_id) AS total_funcionarios,\n",
    "    \n",
    "    SUM(\n",
    "        CASE \n",
    "            WHEN f.termination_date IS NOT NULL THEN 1 \n",
    "            ELSE 0 \n",
    "        END\n",
    "    ) AS total_desligados,\n",
    "    \n",
    "    ROUND(\n",
    "        SUM(\n",
    "            CASE \n",
    "                WHEN f.termination_date IS NOT NULL THEN 1 \n",
    "                ELSE 0 \n",
    "            END\n",
    "        ) / COUNT(DISTINCT f.employee_id),\n",
    "        4\n",
    "    ) AS taxa_rotatividade,\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2726617-6836-4497-b030-20bca43d8c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>total_funcionarios</th><th>total_desligados</th><th>taxa_rotatividade</th><th>tempo_medio_empresa</th></tr></thead><tbody><tr><td>Escritório Executivo</td><td>1</td><td>0</td><td>0.0</td><td>4919.0</td></tr><tr><td>Venda</td><td>32</td><td>15</td><td>0.4688</td><td>4046.41</td></tr><tr><td>Engenharia de Software</td><td>10</td><td>9</td><td>0.9</td><td>3626.1</td></tr><tr><td>Produção</td><td>208</td><td>249</td><td>1.1971</td><td>3309.1</td></tr><tr><td>Escritórios administrativos</td><td>10</td><td>9</td><td>0.9</td><td>3482.9</td></tr><tr><td>T.I</td><td>50</td><td>30</td><td>0.6</td><td>3380.66</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Escritório Executivo",
         1,
         0,
         0.0,
         4919.0
        ],
        [
         "Venda",
         32,
         15,
         0.4688,
         4046.41
        ],
        [
         "Engenharia de Software",
         10,
         9,
         0.9,
         3626.1
        ],
        [
         "Produção",
         208,
         249,
         1.1971,
         3309.1
        ],
        [
         "Escritórios administrativos",
         10,
         9,
         0.9,
         3482.9
        ],
        [
         "T.I",
         50,
         30,
         0.6,
         3380.66
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {
             "comment": "Nome do departamento"
            },
            "name": "department",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_funcionarios",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_desligados",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "taxa_rotatividade",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "tempo_medio_empresa",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 61
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\": \"Nome do departamento\"}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_funcionarios",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_desligados",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "taxa_rotatividade",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tempo_medio_empresa",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM gold.turnover_metrics\n",
    "LIMIT 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77299cc2-a81b-4710-9938-a8cc62758071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. gold.absenteeism_metrics — Absenteísmo**\n",
    "\n",
    "**Objetivo**: Avaliar faltas e atrasos por departamento.\n",
    "\n",
    "**Estrutura:**\n",
    "\n",
    "department\n",
    "\n",
    "total_ausencias\n",
    "\n",
    "media_ausencias\n",
    "\n",
    "total_atrasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a04fba-b77a-4ac6-8da4-13f71481490b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "num_affected_rows",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "num_inserted_rows",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 62
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE gold.absenteeism_metrics AS\n",
    "SELECT\n",
    "    d.department_name AS department,\n",
    "    COUNT(DISTINCT f.employee_id) AS total_funcionarios,\n",
    "    SUM(f.absences) AS total_ausencias,\n",
    "    ROUND(\n",
    "        AVG(f.absences),\n",
    "        2\n",
    "    ) AS media_ausencias_por_funcionario,\n",
    "    SUM(f.days_late_last_30) AS total_atrasos_ultimos_30_dias\n",
    "FROM silver.fact_employee f\n",
    "JOIN silver.dim_department d\n",
    "    ON f.department_id = d.department_id\n",
    "GROUP BY d.department_name;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a633a0-d700-4e77-ba8c-9f7fa5d84964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>total_funcionarios</th><th>total_ausencias</th><th>media_ausencias_por_funcionario</th><th>total_atrasos_ultimos_30_dias</th></tr></thead><tbody><tr><td>Escritório Executivo</td><td>1</td><td>50</td><td>10.0</td><td>0</td></tr><tr><td>Venda</td><td>32</td><td>1800</td><td>11.25</td><td>110</td></tr><tr><td>Engenharia de Software</td><td>10</td><td>460</td><td>9.2</td><td>20</td></tr><tr><td>Produção</td><td>208</td><td>10590</td><td>10.18</td><td>480</td></tr><tr><td>Escritórios administrativos</td><td>10</td><td>410</td><td>8.2</td><td>0</td></tr><tr><td>T.I</td><td>50</td><td>2610</td><td>10.44</td><td>35</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Escritório Executivo",
         1,
         50,
         10.0,
         0
        ],
        [
         "Venda",
         32,
         1800,
         11.25,
         110
        ],
        [
         "Engenharia de Software",
         10,
         460,
         9.2,
         20
        ],
        [
         "Produção",
         208,
         10590,
         10.18,
         480
        ],
        [
         "Escritórios administrativos",
         10,
         410,
         8.2,
         0
        ],
        [
         "T.I",
         50,
         2610,
         10.44,
         35
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {
             "comment": "Nome do departamento"
            },
            "name": "department",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_funcionarios",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_ausencias",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "media_ausencias_por_funcionario",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "total_atrasos_ultimos_30_dias",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 63
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\": \"Nome do departamento\"}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_funcionarios",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_ausencias",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "media_ausencias_por_funcionario",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total_atrasos_ultimos_30_dias",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM gold.absenteeism_metrics\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71edc22b-88ce-4f21-9018-198fa9cb96ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**3.gold.performance_metrics — Desempenho**\n",
    "\n",
    "**Objetivo:**Analisar a distribuição de desempenho e sua relação com ausências.\n",
    "\n",
    "**Estrutura:**\n",
    "\n",
    "department\n",
    "\n",
    "performance_rating\n",
    "\n",
    "qtd_funcionarios\n",
    "\n",
    "media_ausencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38dfe16a-2f6a-4122-9891-f45599e7893b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "num_affected_rows",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "num_inserted_rows",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 64
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE gold.performance_metrics AS\n",
    "SELECT\n",
    "    d.department_name AS department,\n",
    "    f.performance_score,\n",
    "    COUNT(DISTINCT f.employee_id) AS total_funcionarios,\n",
    "    SUM(f.absences) AS total_ausencias,\n",
    "    SUM(f.days_late_last_30) AS total_atrasos_ultimos_30_dias\n",
    "FROM silver.fact_employee f\n",
    "JOIN silver.dim_department d\n",
    "    ON f.department_id = d.department_id\n",
    "GROUP BY\n",
    "    d.department_name,\n",
    "    f.performance_score;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827c5142-698c-4396-9788-a2902eeab63b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"department\":178},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766261152841}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_5dffefe8\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_76d44144\",\"enabled\":true,\"columnId\":\"department\",\"dataType\":\"string\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1766261158884}],\"syncTimestamp\":1766261158884}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>performance_score</th><th>total_funcionarios</th><th>total_ausencias</th><th>total_atrasos_ultimos_30_dias</th></tr></thead><tbody><tr><td>Engenharia de Software</td><td>Atende totalmente</td><td>7</td><td>295</td><td>0</td></tr><tr><td>T.I</td><td>PMD</td><td>1</td><td>10</td><td>15</td></tr><tr><td>Venda</td><td>Precisa de melhorias</td><td>1</td><td>80</td><td>10</td></tr><tr><td>Engenharia de Software</td><td>Precisa de melhorias</td><td>1</td><td>95</td><td>20</td></tr><tr><td>Produção</td><td>Precisa de melhorias</td><td>15</td><td>780</td><td>290</td></tr><tr><td>Venda</td><td>Excede</td><td>2</td><td>100</td><td>0</td></tr><tr><td>T.I</td><td>Excede</td><td>6</td><td>370</td><td>0</td></tr><tr><td>Engenharia de Software</td><td>Excede</td><td>2</td><td>70</td><td>0</td></tr><tr><td>Escritório Executivo</td><td>Atende totalmente</td><td>1</td><td>50</td><td>0</td></tr><tr><td>Venda</td><td>PMD</td><td>4</td><td>170</td><td>75</td></tr><tr><td>T.I</td><td>Precisa de melhorias</td><td>1</td><td>65</td><td>20</td></tr><tr><td>Produção</td><td>Excede</td><td>27</td><td>1400</td><td>0</td></tr><tr><td>Venda</td><td>Atende totalmente</td><td>25</td><td>1450</td><td>25</td></tr><tr><td>Produção</td><td>Atende totalmente</td><td>158</td><td>8050</td><td>0</td></tr><tr><td>Produção</td><td>PMD</td><td>8</td><td>360</td><td>190</td></tr><tr><td>T.I</td><td>Atende totalmente</td><td>42</td><td>2165</td><td>0</td></tr><tr><td>Escritórios administrativos</td><td>Atende totalmente</td><td>10</td><td>410</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Engenharia de Software",
         "Atende totalmente",
         7,
         295,
         0
        ],
        [
         "T.I",
         "PMD",
         1,
         10,
         15
        ],
        [
         "Venda",
         "Precisa de melhorias",
         1,
         80,
         10
        ],
        [
         "Engenharia de Software",
         "Precisa de melhorias",
         1,
         95,
         20
        ],
        [
         "Produção",
         "Precisa de melhorias",
         15,
         780,
         290
        ],
        [
         "Venda",
         "Excede",
         2,
         100,
         0
        ],
        [
         "T.I",
         "Excede",
         6,
         370,
         0
        ],
        [
         "Engenharia de Software",
         "Excede",
         2,
         70,
         0
        ],
        [
         "Escritório Executivo",
         "Atende totalmente",
         1,
         50,
         0
        ],
        [
         "Venda",
         "PMD",
         4,
         170,
         75
        ],
        [
         "T.I",
         "Precisa de melhorias",
         1,
         65,
         20
        ],
        [
         "Produção",
         "Excede",
         27,
         1400,
         0
        ],
        [
         "Venda",
         "Atende totalmente",
         25,
         1450,
         25
        ],
        [
         "Produção",
         "Atende totalmente",
         158,
         8050,
         0
        ],
        [
         "Produção",
         "PMD",
         8,
         360,
         190
        ],
        [
         "T.I",
         "Atende totalmente",
         42,
         2165,
         0
        ],
        [
         "Escritórios administrativos",
         "Atende totalmente",
         10,
         410,
         0
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {
             "comment": "Nome do departamento"
            },
            "name": "department",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {
             "comment": "Avaliação de desempenho"
            },
            "name": "performance_score",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_funcionarios",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_ausencias",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_atrasos_ultimos_30_dias",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 65
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\": \"Nome do departamento\"}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\": \"Avalia\\u00e7\\u00e3o de desempenho\"}",
         "name": "performance_score",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_funcionarios",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_ausencias",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_atrasos_ultimos_30_dias",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM gold.performance_metrics\n",
    "LIMIT 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f86dca6-9e47-4672-b73e-fb7d893ebdd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. gold.satisfaction_metrics — Satisfação**\n",
    "\n",
    "**Objetivo:**Comparar satisfação média por departamento e status.\n",
    "\n",
    "**Estrutura:**\n",
    "\n",
    "department\n",
    "\n",
    "status_funcionario\n",
    "\n",
    "media_satisfacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15b05dd8-91a9-48eb-9fcc-7815d457d345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "num_affected_rows",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "num_inserted_rows",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 66
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE gold.satisfaction_metrics AS\n",
    "SELECT\n",
    "    d.department_name AS department,\n",
    "    CASE \n",
    "        WHEN f.termination_date IS NULL THEN 'Ativo'\n",
    "        ELSE 'Desligado'\n",
    "    END AS status_funcionario,\n",
    "    COUNT(DISTINCT f.employee_id) AS total_funcionarios,\n",
    "    ROUND(AVG(1 + (f.employee_satisfaction - 43) * 4.0 / 457), 2) AS media_satisfacao\n",
    "FROM silver.fact_employee f\n",
    "JOIN silver.dim_department d\n",
    "    ON f.department_id = d.department_id\n",
    "GROUP BY\n",
    "    d.department_name,\n",
    "    CASE \n",
    "        WHEN f.termination_date IS NULL THEN 'Ativo'\n",
    "        ELSE 'Desligado'\n",
    "    END;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c85bc0ea-53de-4586-bf70-8083b18676ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>status_funcionario</th><th>total_funcionarios</th><th>media_satisfacao</th></tr></thead><tbody><tr><td>T.I</td><td>Desligado</td><td>10</td><td>4.47</td></tr><tr><td>Venda</td><td>Ativo</td><td>27</td><td>3.89</td></tr><tr><td>Engenharia de Software</td><td>Desligado</td><td>3</td><td>4.04</td></tr><tr><td>Engenharia de Software</td><td>Ativo</td><td>7</td><td>4.23</td></tr><tr><td>Escritórios administrativos</td><td>Ativo</td><td>7</td><td>4.69</td></tr><tr><td>T.I</td><td>Ativo</td><td>40</td><td>4.21</td></tr><tr><td>Escritório Executivo</td><td>Ativo</td><td>1</td><td>4.85</td></tr><tr><td>Produção</td><td>Desligado</td><td>83</td><td>4.21</td></tr><tr><td>Produção</td><td>Ativo</td><td>125</td><td>4.25</td></tr><tr><td>Escritórios administrativos</td><td>Desligado</td><td>3</td><td>3.89</td></tr><tr><td>Venda</td><td>Desligado</td><td>5</td><td>3.79</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "T.I",
         "Desligado",
         10,
         "4.47"
        ],
        [
         "Venda",
         "Ativo",
         27,
         "3.89"
        ],
        [
         "Engenharia de Software",
         "Desligado",
         3,
         "4.04"
        ],
        [
         "Engenharia de Software",
         "Ativo",
         7,
         "4.23"
        ],
        [
         "Escritórios administrativos",
         "Ativo",
         7,
         "4.69"
        ],
        [
         "T.I",
         "Ativo",
         40,
         "4.21"
        ],
        [
         "Escritório Executivo",
         "Ativo",
         1,
         "4.85"
        ],
        [
         "Produção",
         "Desligado",
         83,
         "4.21"
        ],
        [
         "Produção",
         "Ativo",
         125,
         "4.25"
        ],
        [
         "Escritórios administrativos",
         "Desligado",
         3,
         "3.89"
        ],
        [
         "Venda",
         "Desligado",
         5,
         "3.79"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {
             "comment": "Nome do departamento"
            },
            "name": "department",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "status_funcionario",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_funcionarios",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "media_satisfacao",
            "nullable": true,
            "type": "decimal(16,2)"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 67
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\": \"Nome do departamento\"}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status_funcionario",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_funcionarios",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "media_satisfacao",
         "type": "\"decimal(16,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM gold.satisfaction_metrics\n",
    "LIMIT 20;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeb82071-eb62-40d4-99be-7db3b6dd2ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. gold.diversity_metrics — Diversidade**\n",
    "\n",
    "**Objetivo:**Avaliar distribuição racial por departamento.\n",
    "\n",
    "**Estrutura:**\n",
    "\n",
    "department\n",
    "\n",
    "race\n",
    "\n",
    "qtd_funcionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67387310-ebc3-4187-80b9-016604d0282a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "num_affected_rows",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "num_inserted_rows",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 68
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE gold.diversity_metrics AS\n",
    "SELECT\n",
    "    d.department_name             AS department,\n",
    "    e.race                        AS race,\n",
    "    e.gender                      AS gender,\n",
    "    e.hispanic_latino             AS latino,\n",
    "    COUNT(DISTINCT f.employee_id) AS total_funcionarios\n",
    "FROM silver.fact_employee f\n",
    "JOIN silver.dim_employee e\n",
    "    ON f.employee_id = e.employee_id\n",
    "JOIN silver.dim_department d\n",
    "    ON f.department_id = d.department_id\n",
    "GROUP BY\n",
    "    d.department_name,\n",
    "    e.race,\n",
    "    e.gender,\n",
    "    e.hispanic_latino;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9369937f-635b-4f6b-bf95-1d54934dd121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>race</th><th>gender</th><th>latino</th><th>total_funcionarios</th></tr></thead><tbody><tr><td>Venda</td><td>Negro ou afro-americano</td><td>H</td><td>Não</td><td>4</td></tr><tr><td>Venda</td><td>Índio americano ou nativo do Alasca</td><td>H</td><td>Não</td><td>1</td></tr><tr><td>Venda</td><td>Branco</td><td>H</td><td>Não</td><td>7</td></tr><tr><td>T.I</td><td>Negro ou afro-americano</td><td>H</td><td>Sim</td><td>1</td></tr><tr><td>Engenharia de Software</td><td>Negro ou afro-americano</td><td>M</td><td>Não</td><td>2</td></tr><tr><td>Produção</td><td>Negro ou afro-americano</td><td>H</td><td>Sim</td><td>1</td></tr><tr><td>Venda</td><td>Negro ou afro-americano</td><td>H</td><td>Sim</td><td>1</td></tr><tr><td>Engenharia de Software</td><td>Negro ou afro-americano</td><td>H</td><td>Não</td><td>2</td></tr><tr><td>Produção</td><td>Branco</td><td>H</td><td>Sim</td><td>7</td></tr><tr><td>Escritórios administrativos</td><td>Branco</td><td>M</td><td>Não</td><td>2</td></tr><tr><td>Venda</td><td>Duas ou mais corridas</td><td>M</td><td>Não</td><td>2</td></tr><tr><td>Venda</td><td>Branco</td><td>M</td><td>Não</td><td>6</td></tr><tr><td>T.I</td><td>Negro ou afro-americano</td><td>M</td><td>Não</td><td>5</td></tr><tr><td>Produção</td><td>Negro ou afro-americano</td><td>M</td><td>Não</td><td>26</td></tr><tr><td>Produção</td><td>Branco</td><td>M</td><td>Sim</td><td>3</td></tr><tr><td>Produção</td><td>Duas ou mais corridas</td><td>H</td><td>Não</td><td>2</td></tr><tr><td>Escritórios administrativos</td><td>Negro ou afro-americano</td><td>M</td><td>Não</td><td>4</td></tr><tr><td>Escritórios administrativos</td><td>Branco</td><td>H</td><td>Não</td><td>2</td></tr><tr><td>Produção</td><td>Asiático</td><td>M</td><td>Não</td><td>13</td></tr><tr><td>Produção</td><td>Duas ou mais corridas</td><td>M</td><td>Não</td><td>3</td></tr><tr><td>Produção</td><td>Índio americano ou nativo do Alasca</td><td>M</td><td>Não</td><td>2</td></tr><tr><td>T.I</td><td>Branco</td><td>H</td><td>Não</td><td>14</td></tr><tr><td>T.I</td><td>Asiático</td><td>M</td><td>Não</td><td>3</td></tr><tr><td>Engenharia de Software</td><td>Branco</td><td>M</td><td>Não</td><td>3</td></tr><tr><td>T.I</td><td>Branco</td><td>M</td><td>Não</td><td>10</td></tr><tr><td>Venda</td><td>Duas ou mais corridas</td><td>H</td><td>Não</td><td>3</td></tr><tr><td>Escritório Executivo</td><td>Branco</td><td>M</td><td>Sim</td><td>1</td></tr><tr><td>Produção</td><td>Negro ou afro-americano</td><td>H</td><td>Não</td><td>14</td></tr><tr><td>T.I</td><td>Negro ou afro-americano</td><td>H</td><td>Não</td><td>8</td></tr><tr><td>Produção</td><td>Asiático</td><td>H</td><td>Não</td><td>8</td></tr><tr><td>Venda</td><td>Asiático</td><td>H</td><td>Não</td><td>1</td></tr><tr><td>Produção</td><td>Hispânico</td><td>H</td><td>Sim</td><td>1</td></tr><tr><td>Engenharia de Software</td><td>Branco</td><td>H</td><td>Não</td><td>2</td></tr><tr><td>Engenharia de Software</td><td>Asiático</td><td>M</td><td>Não</td><td>1</td></tr><tr><td>Produção</td><td>Duas ou mais corridas</td><td>M</td><td>Sim</td><td>1</td></tr><tr><td>Produção</td><td>Negro ou afro-americano</td><td>M</td><td>Sim</td><td>4</td></tr><tr><td>T.I</td><td>Negro ou afro-americano</td><td>M</td><td>Sim</td><td>1</td></tr><tr><td>T.I</td><td>Asiático</td><td>H</td><td>Não</td><td>3</td></tr><tr><td>Escritórios administrativos</td><td>Negro ou afro-americano</td><td>H</td><td>Não</td><td>2</td></tr><tr><td>T.I</td><td>Branco</td><td>M</td><td>Sim</td><td>3</td></tr><tr><td>T.I</td><td>Branco</td><td>H</td><td>Sim</td><td>2</td></tr><tr><td>Venda</td><td>Negro ou afro-americano</td><td>M</td><td>Não</td><td>5</td></tr><tr><td>Produção</td><td>Branco</td><td>M</td><td>Não</td><td>74</td></tr><tr><td>Venda</td><td>Branco</td><td>M</td><td>Sim</td><td>2</td></tr><tr><td>Produção</td><td>Branco</td><td>H</td><td>Não</td><td>49</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Venda",
         "Negro ou afro-americano",
         "H",
         "Não",
         4
        ],
        [
         "Venda",
         "Índio americano ou nativo do Alasca",
         "H",
         "Não",
         1
        ],
        [
         "Venda",
         "Branco",
         "H",
         "Não",
         7
        ],
        [
         "T.I",
         "Negro ou afro-americano",
         "H",
         "Sim",
         1
        ],
        [
         "Engenharia de Software",
         "Negro ou afro-americano",
         "M",
         "Não",
         2
        ],
        [
         "Produção",
         "Negro ou afro-americano",
         "H",
         "Sim",
         1
        ],
        [
         "Venda",
         "Negro ou afro-americano",
         "H",
         "Sim",
         1
        ],
        [
         "Engenharia de Software",
         "Negro ou afro-americano",
         "H",
         "Não",
         2
        ],
        [
         "Produção",
         "Branco",
         "H",
         "Sim",
         7
        ],
        [
         "Escritórios administrativos",
         "Branco",
         "M",
         "Não",
         2
        ],
        [
         "Venda",
         "Duas ou mais corridas",
         "M",
         "Não",
         2
        ],
        [
         "Venda",
         "Branco",
         "M",
         "Não",
         6
        ],
        [
         "T.I",
         "Negro ou afro-americano",
         "M",
         "Não",
         5
        ],
        [
         "Produção",
         "Negro ou afro-americano",
         "M",
         "Não",
         26
        ],
        [
         "Produção",
         "Branco",
         "M",
         "Sim",
         3
        ],
        [
         "Produção",
         "Duas ou mais corridas",
         "H",
         "Não",
         2
        ],
        [
         "Escritórios administrativos",
         "Negro ou afro-americano",
         "M",
         "Não",
         4
        ],
        [
         "Escritórios administrativos",
         "Branco",
         "H",
         "Não",
         2
        ],
        [
         "Produção",
         "Asiático",
         "M",
         "Não",
         13
        ],
        [
         "Produção",
         "Duas ou mais corridas",
         "M",
         "Não",
         3
        ],
        [
         "Produção",
         "Índio americano ou nativo do Alasca",
         "M",
         "Não",
         2
        ],
        [
         "T.I",
         "Branco",
         "H",
         "Não",
         14
        ],
        [
         "T.I",
         "Asiático",
         "M",
         "Não",
         3
        ],
        [
         "Engenharia de Software",
         "Branco",
         "M",
         "Não",
         3
        ],
        [
         "T.I",
         "Branco",
         "M",
         "Não",
         10
        ],
        [
         "Venda",
         "Duas ou mais corridas",
         "H",
         "Não",
         3
        ],
        [
         "Escritório Executivo",
         "Branco",
         "M",
         "Sim",
         1
        ],
        [
         "Produção",
         "Negro ou afro-americano",
         "H",
         "Não",
         14
        ],
        [
         "T.I",
         "Negro ou afro-americano",
         "H",
         "Não",
         8
        ],
        [
         "Produção",
         "Asiático",
         "H",
         "Não",
         8
        ],
        [
         "Venda",
         "Asiático",
         "H",
         "Não",
         1
        ],
        [
         "Produção",
         "Hispânico",
         "H",
         "Sim",
         1
        ],
        [
         "Engenharia de Software",
         "Branco",
         "H",
         "Não",
         2
        ],
        [
         "Engenharia de Software",
         "Asiático",
         "M",
         "Não",
         1
        ],
        [
         "Produção",
         "Duas ou mais corridas",
         "M",
         "Sim",
         1
        ],
        [
         "Produção",
         "Negro ou afro-americano",
         "M",
         "Sim",
         4
        ],
        [
         "T.I",
         "Negro ou afro-americano",
         "M",
         "Sim",
         1
        ],
        [
         "T.I",
         "Asiático",
         "H",
         "Não",
         3
        ],
        [
         "Escritórios administrativos",
         "Negro ou afro-americano",
         "H",
         "Não",
         2
        ],
        [
         "T.I",
         "Branco",
         "M",
         "Sim",
         3
        ],
        [
         "T.I",
         "Branco",
         "H",
         "Sim",
         2
        ],
        [
         "Venda",
         "Negro ou afro-americano",
         "M",
         "Não",
         5
        ],
        [
         "Produção",
         "Branco",
         "M",
         "Não",
         74
        ],
        [
         "Venda",
         "Branco",
         "M",
         "Sim",
         2
        ],
        [
         "Produção",
         "Branco",
         "H",
         "Não",
         49
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {
             "comment": "Nome do departamento"
            },
            "name": "department",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "race",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "gender",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "latino",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_funcionarios",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 69
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\": \"Nome do departamento\"}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "race",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "latino",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_funcionarios",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM gold.diversity_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dd7bee3-adb2-4767-802f-f4ccc7f7ec74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**6. gold.salary_metrics — Massa Salarial**\n",
    "\n",
    "**Objetivo**: Analisar massa salarial e média salarial por departamento e cargo.\n",
    "\n",
    "**Estrutura:**\n",
    "\n",
    "department\n",
    "\n",
    "job_title\n",
    "\n",
    "massa_salarial\n",
    "\n",
    "salario_medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "999d1d13-4e28-4705-bf0f-2cb53946d2fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "num_affected_rows",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "num_inserted_rows",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 70
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE gold.salary_metrics AS\n",
    "SELECT\n",
    "    d.department_name                              AS department,\n",
    "    COUNT(DISTINCT f.employee_id)                  AS total_funcionarios,\n",
    "    ROUND(SUM(f.salary), 2)                        AS massa_salarial_total,\n",
    "    ROUND(AVG(f.salary), 2)                        AS salario_medio,\n",
    "    ROUND(MIN(f.salary), 2)                        AS salario_minimo,\n",
    "    ROUND(MAX(f.salary), 2)                        AS salario_maximo\n",
    "FROM silver.fact_employee f\n",
    "JOIN silver.dim_department d\n",
    "    ON f.department_id = d.department_id\n",
    "GROUP BY d.department_name;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d3d7941-b4a7-49e4-995c-666fbf275de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>department</th><th>total_funcionarios</th><th>massa_salarial_total</th><th>salario_medio</th><th>salario_minimo</th><th>salario_maximo</th></tr></thead><tbody><tr><td>Produção</td><td>208</td><td>6.23665E7</td><td>59967.79</td><td>45046.0</td><td>170500.0</td></tr><tr><td>T.I</td><td>50</td><td>2.426616E7</td><td>97064.64</td><td>50178.0</td><td>220450.0</td></tr><tr><td>Venda</td><td>32</td><td>1.098945E7</td><td>68684.06</td><td>55875.0</td><td>180000.0</td></tr><tr><td>Engenharia de Software</td><td>10</td><td>4807605.0</td><td>96152.1</td><td>77692.0</td><td>108987.0</td></tr><tr><td>Escritórios administrativos</td><td>10</td><td>3647450.0</td><td>72949.0</td><td>49920.0</td><td>106367.0</td></tr><tr><td>Escritório Executivo</td><td>1</td><td>1250000.0</td><td>250000.0</td><td>250000.0</td><td>250000.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Produção",
         208,
         6.23665E7,
         59967.79,
         45046.0,
         170500.0
        ],
        [
         "T.I",
         50,
         2.426616E7,
         97064.64,
         50178.0,
         220450.0
        ],
        [
         "Venda",
         32,
         1.098945E7,
         68684.06,
         55875.0,
         180000.0
        ],
        [
         "Engenharia de Software",
         10,
         4807605.0,
         96152.1,
         77692.0,
         108987.0
        ],
        [
         "Escritórios administrativos",
         10,
         3647450.0,
         72949.0,
         49920.0,
         106367.0
        ],
        [
         "Escritório Executivo",
         1,
         1250000.0,
         250000.0,
         250000.0,
         250000.0
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {
             "comment": "Nome do departamento"
            },
            "name": "department",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "total_funcionarios",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "massa_salarial_total",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "salario_medio",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "salario_minimo",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "salario_maximo",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 71
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\": \"Nome do departamento\"}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_funcionarios",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "massa_salarial_total",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "salario_medio",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "salario_minimo",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "salario_maximo",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM gold.salary_metrics\n",
    "ORDER BY massa_salarial_total DESC;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7450327388039454,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "7_etl_db_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}